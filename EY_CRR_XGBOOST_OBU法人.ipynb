{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBU XGBoost\n",
    "\n",
    "Update\n",
    "1. 分析使用一年份資料，僅保留客戶最新資訊欄位\n",
    "2. 與兆豐討論後移除變數如下\n",
    "    - REPORT_FG\n",
    "    - CDD_STATUS\n",
    "    - RISK_LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入Module\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve #P-R Curve\n",
    "from sklearn.model_selection import GridSearchCV #Hyperparameters\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "import hyperopt\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Import & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料輸入\n",
    "\n",
    "# EY補充\n",
    "# 此處路徑需更新為最新資料存放路徑\n",
    "\n",
    "OBU = pd.read_csv(r\"/home/bigdata93601/DB Result/OBU_result_one_year.csv\", dtype=str)\n",
    "OBU['CYC_MN'] = pd.to_datetime(OBU['CYC_MN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除FINAL_CRR為空值的row\n",
    "OBU = OBU[OBU['FINAL_CRR'].notna()]\n",
    "OBU.reset_index(inplace=True)\n",
    "\n",
    "# CUST_DUP_NO 空值補0\n",
    "OBU['CUST_DUP_NO'] = OBU['CUST_DUP_NO'].fillna('0')\n",
    "\n",
    "# SAR: 非 Y即 N\n",
    "OBU['SAR'] = np.where(OBU['SAR'] == 'Y', 'Y', 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Duplicated data (New Version)\n",
    "目的: 僅保留在資訊層面最新之帳戶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBU = OBU.sort_values('CYC_MN', ascending=False).drop_duplicates(subset=['CUST_ID', 'CUST_DUP_NO'], keep='first')\n",
    "OBU.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBU['CYC_MN'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 變數轉換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 類別型變數  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EY補充\n",
    "# 新增欄位的情況下，將類別型變數轉換為Y/N的資料格式。\n",
    "# 下面範例將以df代表輸入資料，NEW_COLUMN代表新增欄位\n",
    "# \n",
    "# 範例一\n",
    "# 新增欄位的資料內容為[\"V\", Nan, \"V\", \"V\", Nan, ... , \"V\"]；欲將V轉換為Y，空值(Nan)轉換為N。\n",
    "# df[\"NEW_COLUMN\"] = np.where(df[\"NEW_COLUMN\"] == \"V\", \"Y\", \"N\")\n",
    "#\n",
    "# 範例二\n",
    "# 新增欄位的資料內容為[\"Y\", \"DELAY\", \"Y\", Nan, \"Y\", ... , \"N\"]；欲將非Y/N轉換為N，其餘資料維持原狀。\n",
    "# df[\"NEW_COLUMN\"] = np.where(df[\"NEW_COLUMN\"].isin([\"Y\", \"N\"], df[\"NEW_COLUMN\"], \"N\"))\n",
    "#\n",
    "# 範例三\n",
    "# 新增欄位的資料內容為[\"01\", \"02\", \"03\", \"04\", ..., \"01\"]；欲將01-02轉換為Y，03-04轉換為N。\n",
    "# df[\"NEW_COLUMN\"] = df[\"NEW_COLUMN\"].map({\"01\": \"Y\", \"02\": \"Y\", \"03\": \"N\", \"04\": \"N\"})\n",
    "\n",
    "\n",
    "# 需先經過轉換的變數\n",
    "# 1. CMFCUS1_AML_BUSINESS\n",
    "def apply_CMFCUS1_AML_BUSINESS(series):\n",
    "    input = series['CMFCUS1_AML_BUSINESS']\n",
    "    if (input in ['01', '02', '03', '04', '05', '06', '07', '08']):\n",
    "        return 'Y'\n",
    "    else:\n",
    "        return 'N'\n",
    "\n",
    "OBU['CMFCUS1_AML_BUSINESS'] = OBU.apply(apply_CMFCUS1_AML_BUSINESS, axis=1)\n",
    "\n",
    "\n",
    "for i in ['DP_FG', 'LN_FG', 'IX_FG', 'BD_FG', 'FD_FG', 'WM_FG', 'TR_FG', 'EB_FG', 'CD_FG', 'OT_FG']:\n",
    "    OBU[i] = np.where(OBU[i] == 'V', 'Y', 'N')\n",
    "\n",
    "    \n",
    "\n",
    "# 2. CMFCUS25_AE_TYPE\n",
    "# Turn Na into 'N'; other into 'Y'\n",
    "OBU['CMFCUS25_AE_TYPE'] = np.where((OBU[\"CMFCUS25_AE_TYPE\"].isin(['1', '2', '3', '4', '5', '6', '7'])), 'Y', 'N')\n",
    "\n",
    "\n",
    "\n",
    "# 全部Categorical變數\n",
    "# 非 Y or N 即 N\n",
    "\n",
    "X_Categorical = ['CMFCUS1_VIP_CODE', 'CMFCUS1_BUSINESS_FLAG', 'CMFCUS1_NOTAX_FLAG', 'CMFCUS1_FINANCIAL_ACT', \n",
    "                 'CMFCUS1_AML_BUSINESS', 'DP_FG', 'LN_FG', 'IX_FG', 'BD_FG', 'FD_FG', 'WM_FG', 'TR_FG', 'EB_FG', \n",
    "                 'CD_FG', 'OT_FG', 'TRUST_YN', 'CONFIRM_YN', 'COMPLEX_CS_FG', 'AUTHORIZED', 'BEARER_SHARE', \n",
    "                 'ISSUE_BEARER', 'SOLE_CORP', 'TRUST_HOLDER', 'CUST_PANA', 'CUST_THIRD', 'CUST_ADVRS', \n",
    "                 'CUST_BAHA', 'CUST_PARA', 'CMFCUS25_AE_TYPE', 'CMFCUS25_FOREIGN_COMPANY', 'CMFCUS25_FOREIGN_ENTITY', \n",
    "                 'CMFCUS25_CERTI_FLAG', 'CMFCUS25_TAXFREE_FLAG', 'CMFCUS25_CREATIVE_FLAG', 'CMFCUS25_OSU_FLAG', \n",
    "                 'CMFCUS25_PUBLIC_CMPY']\n",
    "\n",
    "for x in X_Categorical:\n",
    "    OBU[x] = np.where(~OBU[x].isin(['Y', 'N']), 'N', OBU[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將Y/N轉換為1/0以方便後面模型執行計算\n",
    "\n",
    "# Turn 'Y', 'N' into 1, 0\n",
    "for col in X_Categorical:\n",
    "    OBU[col] = LabelEncoder().fit_transform(OBU[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 數值型變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需先經過轉換的變數\n",
    "# 1. PEP_COUNT\n",
    "test = OBU[(OBU['PEP_1'] == 'Y') | (OBU['PEP_2'] == 'Y') | (OBU['PEP_3'] == 'Y') | (OBU['PEP_4'] == 'Y')][['PEP_1', 'PEP_2', 'PEP_3', 'PEP_4']]\n",
    "test2 = test.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "OBU['PEP_COUNT'] = 0\n",
    "OBU.loc[test2.index, 'PEP_COUNT'] = test2['Y']\n",
    "\n",
    "# 2. REL_ADVERS_COUNT\n",
    "test = OBU[(OBU['REL_ADVRS_1'] == 'Y') | (OBU['REL_ADVRS_2'] == 'Y') | (OBU['REL_ADVRS_3'] == 'Y') | (OBU['REL_ADVRS_4'] == 'Y') | (OBU['REL_ADVRS_5'] == 'Y') | \n",
    "          (OBU['REL_ADVRS_6'] == 'Y') | (OBU['REL_ADVRS_7'] == 'Y') | (OBU['REL_ADVRS_8'] == 'Y') | (OBU['REL_ADVRS_9'] == 'Y') | (OBU['REL_ADVRS_10'] == 'Y') |\n",
    "          (OBU['REL_ADVRS_11'] == 'Y') | (OBU['REL_ADVRS_12'] == 'Y') | (OBU['REL_ADVRS_13'] == 'Y') | (OBU['REL_ADVRS_14'] == 'Y') | (OBU['REL_ADVRS_15'] == 'Y') | \n",
    "          (OBU['REL_ADVRS_16'] == 'Y') | (OBU['REL_ADVRS_17'] == 'Y') | (OBU['REL_ADVRS_18'] == 'Y') | (OBU['REL_ADVRS_19'] == 'Y') | (OBU['REL_ADVRS_20'] == 'Y')\n",
    "          ][['REL_ADVRS_1', 'REL_ADVRS_2', 'REL_ADVRS_3', 'REL_ADVRS_4', 'REL_ADVRS_5', 'REL_ADVRS_6', 'REL_ADVRS_7', 'REL_ADVRS_8', \n",
    "            'REL_ADVRS_9', 'REL_ADVRS_10', 'REL_ADVRS_11', 'REL_ADVRS_12', 'REL_ADVRS_13', 'REL_ADVRS_14', 'REL_ADVRS_15', 'REL_ADVRS_16', \n",
    "            'REL_ADVRS_17', 'REL_ADVRS_18', 'REL_ADVRS_19', 'REL_ADVRS_20']] \n",
    "test2 = test.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "OBU['REL_ADVERS_COUNT'] = 0\n",
    "OBU.loc[test2.index, 'REL_ADVERS_COUNT'] = test2['Y']\n",
    "\n",
    "# 3. REL_PEPS_COUNT\n",
    "test = OBU[(OBU['REL_PEPS_1'] == 'Y') | (OBU['REL_PEPS_2'] == 'Y') | (OBU['REL_PEPS_3'] == 'Y') | (OBU['REL_PEPS_4'] == 'Y') | (OBU['REL_PEPS_5'] == 'Y') | \n",
    "          (OBU['REL_PEPS_6'] == 'Y') | (OBU['REL_PEPS_7'] == 'Y') | (OBU['REL_PEPS_8'] == 'Y') | (OBU['REL_PEPS_9'] == 'Y') | (OBU['REL_PEPS_10'] == 'Y') |\n",
    "          (OBU['REL_PEPS_11'] == 'Y') | (OBU['REL_PEPS_12'] == 'Y') | (OBU['REL_PEPS_13'] == 'Y') | (OBU['REL_PEPS_14'] == 'Y') | (OBU['REL_PEPS_15'] == 'Y') | \n",
    "          (OBU['REL_PEPS_16'] == 'Y') | (OBU['REL_PEPS_17'] == 'Y') | (OBU['REL_PEPS_18'] == 'Y') | (OBU['REL_PEPS_19'] == 'Y') | (OBU['REL_PEPS_20'] == 'Y')\n",
    "          ][['REL_PEPS_1', 'REL_PEPS_2', 'REL_PEPS_3', 'REL_PEPS_4', 'REL_PEPS_5', 'REL_PEPS_6', 'REL_PEPS_7', 'REL_PEPS_8', \n",
    "            'REL_PEPS_9', 'REL_PEPS_10', 'REL_PEPS_11', 'REL_PEPS_12', 'REL_PEPS_13', 'REL_PEPS_14', 'REL_PEPS_15', 'REL_PEPS_16', \n",
    "            'REL_PEPS_17', 'REL_PEPS_18', 'REL_PEPS_19', 'REL_PEPS_20']] \n",
    "test2 = test.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "OBU['REL_PEPS_COUNT'] = 0\n",
    "OBU.loc[test2.index, 'REL_PEPS_COUNT'] = test2['Y']\n",
    "\n",
    "\n",
    "# 全部 Numeric 變數\n",
    "# 非 數字 即 0\n",
    "X_Numerical = ['CMFCUS1_ADR_CNT', 'CMFCUS1_MPHONE_CNT', 'CMFCUS1_OPHONE_CNT', \n",
    "               'PEP_COUNT', 'REL_ADVERS_COUNT', 'REL_PEPS_COUNT']\n",
    "\n",
    "for col in X_Numerical:\n",
    "    OBU[col] = OBU[col].fillna(0)\n",
    "    OBU[col] = OBU[col].astype(int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 順序型變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EY補充\n",
    "# 新增欄位的情況下，將順序型變數轉換為數字順序(1, 2, 3,...)。\n",
    "# 下面範例將以df代表輸入資料，NEW_COLUMN代表新增欄位\n",
    "# \n",
    "# 範例一\n",
    "# 新增欄位的資料內容為[\"L\", \"M\", \"H\", Nan, ..., \"H\"]；欲將H/M/L轉換為3/2/1，空值(Nan)轉換為1。\n",
    "# df[\"NEW_COLUMN\"] = df[\"NEW_COLUMN\"].map({\"H\": \"3\", \"M\": \"2\", \"L\": \"1\"}).fillna(\"1\")\n",
    "\n",
    "\n",
    "# 需先經過轉換的變數\n",
    "# Remember that changing NA value into the lowest ranking in ordinal variables might cause issue and affect the model performation\n",
    "# It should be adjusted to dummy variables in the second proposal.\n",
    "\n",
    "# 1. CMFCUS1_BUSINESS_CODE\n",
    "BUSINESS_CODE_TABLE = pd.read_csv(r\"/home/bigdata93601/DB Result/BUSINESS_CODE_TABLE.csv\", dtype=str) #行業編號對照風險等級表\n",
    "\n",
    "# Create dictionary\n",
    "BUSINESS_CODE_dict = {'L': BUSINESS_CODE_TABLE[BUSINESS_CODE_TABLE['RISK']=='L']['CODE'].tolist(),\n",
    "                      'M': BUSINESS_CODE_TABLE[BUSINESS_CODE_TABLE['RISK']=='M']['CODE'].tolist(),\n",
    "                      'H': BUSINESS_CODE_TABLE[BUSINESS_CODE_TABLE['RISK']=='H']['CODE'].tolist()}\n",
    "# Inverse dictionary\n",
    "BUSINESS_CODE_dict = {c: r for r, code in BUSINESS_CODE_dict.items() for c in code}\n",
    "\n",
    "OBU['CMFCUS1_BUSINESS_CODE'].replace(BUSINESS_CODE_dict, inplace=True)\n",
    "# Nan轉為L\n",
    "OBU['CMFCUS1_BUSINESS_CODE'] = np.where(~OBU[\"CMFCUS1_BUSINESS_CODE\"].isin(['H', 'M', 'L']), 'L', OBU['CMFCUS1_BUSINESS_CODE'])\n",
    "\n",
    "\n",
    "# 2. LOCATION_CD & HEAD_OFFICE_CD \n",
    "# Create a new column: Nation_Risk to select the higher one between the two columns\n",
    "NationRisk_Table = pd.read_excel(r\"/home/bigdata93601/EY/NationRisk_Table.xlsx\", dtype=str)\n",
    "\n",
    "NationRisk_dict = {'1':NationRisk_Table[NationRisk_Table['洗錢風險評等']=='L']['國家代碼'].tolist(),\n",
    "                   '2': NationRisk_Table[NationRisk_Table['洗錢風險評等']=='ML']['國家代碼'].tolist(),\n",
    "                   '3': NationRisk_Table[NationRisk_Table['洗錢風險評等']=='M']['國家代碼'].tolist(),\n",
    "                   '4': NationRisk_Table[NationRisk_Table['洗錢風險評等']=='MH']['國家代碼'].tolist(),\n",
    "                   '5': NationRisk_Table[NationRisk_Table['洗錢風險評等']=='H']['國家代碼'].tolist()}\n",
    "\n",
    "# Inverse dictionary\n",
    "NationRisk_dict = {c: r for r, code in NationRisk_dict.items() for c in code}\n",
    "\n",
    "OBU['LOCATION_CD'].replace(NationRisk_dict, inplace=True)\n",
    "OBU['HEAD_OFFICE_CD'].replace(NationRisk_dict, inplace=True)\n",
    "# Nan or no matching 轉為H\n",
    "OBU['LOCATION_CD'] = np.where(~OBU['LOCATION_CD'].isin(['1', '2', '3', '4', '5']), '5', OBU['LOCATION_CD'])\n",
    "OBU['HEAD_OFFICE_CD'] = np.where(~OBU['HEAD_OFFICE_CD'].isin(['1', '2', '3', '4', '5']), '5', OBU['HEAD_OFFICE_CD'])\n",
    "\n",
    "OBU['NATION_RISK'] = OBU[['LOCATION_CD', 'HEAD_OFFICE_CD']].max(axis=1).astype(int).astype(str)\n",
    "\n",
    "\n",
    "# 3. AMT_RANGE\n",
    "# Nan 轉為0\n",
    "OBU['AMT_RANGE'] = np.where(~OBU[\"AMT_RANGE\"].isin(['1', '2', '3']), '0', OBU['AMT_RANGE'])\n",
    "\n",
    "# 4. OBU_ANNUAL_INCOME\n",
    "# Nan 轉為0\n",
    "OBU['OBU_ANNUAL_INCOME'] = np.where(~OBU[\"OBU_ANNUAL_INCOME\"].isin(['1', '2', '3']), '0', OBU['OBU_ANNUAL_INCOME'])\n",
    "\n",
    "# 5. L, M, H to number 1, 2, 3\n",
    "def risk_to_number(series):\n",
    "    input = series\n",
    "    if input == 'H':\n",
    "        return '3'\n",
    "    elif input == 'M':\n",
    "        return '2'\n",
    "    else:\n",
    "        return '1'\n",
    "\n",
    "for i in ['CMFCUS1_BUSINESS_CODE', 'JOB_RISK']:\n",
    "    OBU[i] = OBU[i].apply(risk_to_number)\n",
    "\n",
    "# 5. CMFCUS25_SP_RATING\n",
    "OBU['CMFCUS25_SP_RATING'] = OBU['CMFCUS25_SP_RATING'].map({np.nan: 1, 'D': 2, 'SD': 3, 'R': 4, 'CCC-': 5, 'CCC': 6, \n",
    "                                                           'CCC+': 7, 'B-': 8, 'B': 9, 'B+': 10, 'BB-': 11, 'BB': 12, \n",
    "                                                           'BB+': 13, 'BBB-': 14, 'BBB': 15, 'BBB+': 16, 'A-': 17, \n",
    "                                                           'A': 18, 'A+': 19, 'AA-': 20, 'AA': 21, 'AA+': 22, 'AAA': 23})\n",
    "    \n",
    "# 6. CMFCUS25_MOODYS_RATING\n",
    "# Moody's has assigned (P)A1 ratings to the 1 billion preferred securities shelf registration of five Travelers Capital trusts\n",
    "# Here I'll take it as the same level with A1\n",
    "OBU['CMFCUS25_MOODYS_RATING'] = np.where(OBU['CMFCUS25_MOODYS_RATING'] == '(P)A1', 'A1', OBU['CMFCUS25_MOODYS_RATING'])\n",
    "\n",
    "OBU['CMFCUS25_MOODYS_RATING'] = np.where(OBU['CMFCUS25_MOODYS_RATING'] == 'A3-', 'A3', OBU['CMFCUS25_MOODYS_RATING']) #Fix typo\n",
    "\n",
    "OBU['CMFCUS25_MOODYS_RATING'] = OBU['CMFCUS25_MOODYS_RATING'].map({np.nan: 1, 'D': 2, 'C': 3, 'Ca': 4, 'Caa3': 5, 'Caa2': 6, \n",
    "                                                                   'Caa1': 7, 'B3': 8, 'B2': 9, 'B1': 10, 'Ba3': 11, 'Ba2': 12, \n",
    "                                                                   'Ba1': 13, 'Baa3': 14, 'Baa2': 15, 'Baa1': 16, 'A3': 17, \n",
    "                                                                   'A2': 18, 'A1': 19, 'Aa3': 20, 'Aa2': 21, 'Aa1': 22, 'Aaa': 23})\n",
    "\n",
    "\n",
    "# 7. CMFCUS25_SRT_SP_RATING\n",
    "OBU['CMFCUS25_SRT_SP_RATING'] = OBU['CMFCUS25_SRT_SP_RATING'].map({np.nan: 1, 'D': 2, 'C': 3, 'B': 4, 'A-3': 5, \n",
    "                                                                   'A-2': 6, 'A-1': 7, 'A-1+': 8})\n",
    "\n",
    "# 8. CMFCUS25_SRT_MOODYS_RATING\n",
    "OBU['CMFCUS25_SRT_MOODYS_RATING'] = OBU['CMFCUS25_SRT_MOODYS_RATING'].map({np.nan: 1, 'P-3': 2, 'P-2': 3, 'P-1': 4})\n",
    "\n",
    "# 9. CMFCUS25_SRT_FITCH_RATING\n",
    "OBU['CMFCUS25_SRT_FITCH_RATING'] = np.where(OBU['CMFCUS25_SRT_FITCH_RATING'] == 'F2-', 'F2', OBU['CMFCUS25_SRT_FITCH_RATING']) #Fix typo\n",
    "\n",
    "OBU['CMFCUS25_SRT_FITCH_RATING'] = OBU['CMFCUS25_SRT_FITCH_RATING'].map({np.nan: 1, 'D': 2, 'C': 3, 'B': 4, 'F3': 5, \n",
    "                                                                         'F2': 6, 'F1': 7, 'F1+': 8})\n",
    "\n",
    "\n",
    "# 全部Ordinal變數\n",
    "X_Ordinal = ['CMFCUS1_BUSINESS_CODE', 'NATION_RISK', 'AMT_RANGE', \n",
    "             'OBU_ANNUAL_INCOME', 'JOB_RISK', 'CMFCUS25_SP_RATING', 'CMFCUS25_MOODYS_RATING', \n",
    "             'CMFCUS25_SRT_SP_RATING', 'CMFCUS25_SRT_MOODYS_RATING', 'CMFCUS25_SRT_FITCH_RATING']\n",
    "\n",
    "for col in X_Ordinal:\n",
    "    OBU[col] = OBU[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 啞變數(Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EY補充\n",
    "# 新增欄位的情況下，確認啞變數資料內容。\n",
    "# 此階段僅需確保啞變數內容正確，one hot encoding的部分會在後面執行。\n",
    "# 需要注意的地方為若欲將空值作為一個單獨的類別，則建議將空值轉換為字串(EX: \"NA_TYPE\")。\n",
    "#\n",
    "# 下面範例將以df代表輸入資料，NEW_COLUMN代表新增欄位\n",
    "#\n",
    "# 範例一\n",
    "# 新增欄位的資料內容為[\"A\", \"C\", \"1\", \"3\", Nan,..., Nan]；欲將空值(Nan)轉換為\"NA_TYPE\"，其餘內容保持不變。\n",
    "# df[\"NEW_COLUMN\"] = np.where(df[\"NEW_COLUMN\"].isna(), \"NA_TYPE\", df[\"NEW_COLUMN\"])\n",
    "#\n",
    "# 範例二\n",
    "# 新增欄位的資料內容為[\"A\", \"C\", \"B\", \"DELAY\", \"Q\", Nan, ..., \"A\"]；欲將A, B, C, Q保留，其餘內容分類為空值類別。\n",
    "# df[\"NEW_COLUMN\"] = np.where(df[\"NEW_COLUMN\"].isin([\"A\", \"B\", \"C\", \"Q\"]), df[\"NEW_COLUMN\"], \"NA_TYPE\")\n",
    "\n",
    "\n",
    "# 需先經過轉換的變數\n",
    "# 1. CMFCUS1_BIRTH_DATE\n",
    "OBU['CMFCUS1_BIRTH_DATE'] = pd.to_datetime(OBU['CMFCUS1_BIRTH_DATE'], errors='coerce') #if out of bounds then return NAT\n",
    "\n",
    "def calculate_age(born):\n",
    "    today = date.today()\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "\n",
    "OBU['CMFCUS1_BIRTH_DATE'] = OBU['CMFCUS1_BIRTH_DATE'].apply(calculate_age)\n",
    "\n",
    "def apply_age(Series):\n",
    "    input = Series\n",
    "    if input < 3:\n",
    "        return '0-3'\n",
    "    elif (input >= 3) & (input < 5):\n",
    "        return '3-5'\n",
    "    elif (input >= 5) & (input < 10):\n",
    "        return '5-10'\n",
    "    elif input >= 10:\n",
    "        return 'larger_than_10'\n",
    "    else:\n",
    "        'NA_TYPE'\n",
    "\n",
    "OBU['CMFCUS1_BIRTH_DATE'] = OBU['CMFCUS1_BIRTH_DATE'].apply(apply_age)\n",
    "\n",
    "\n",
    "# 2. CMFCUS1_Q_ID\n",
    "# 00, 01, 03, 10, 36, 49, 99都歸於NA_TYPE\n",
    "OBU['CMFCUS1_Q_ID'] = np.where(OBU[\"CMFCUS1_Q_ID\"].isin(['00', '01', '03', '10', '36', '49', '99', 'ZZ', np.nan]), \n",
    "                               'NA_TYPE', OBU[\"CMFCUS1_Q_ID\"])\n",
    "\n",
    "# 3. 'CMFCUS1_PURPOSE', 'CMFCUS1_DERIVATIVE', 'CMFCUS1_TITLE_CODE', 'LN_TYP', 'CONFIRM_TYPE', 'CMFCUS25_BRANCH'\n",
    "# NA改成NA_TYPE\n",
    "for i  in ['CMFCUS1_JOB_TITLE', 'CMFCUS1_PURPOSE', 'CMFCUS1_DERIVATIVE', 'CMFCUS1_TITLE_CODE', 'LN_TYP', 'CONFIRM_TYPE', 'CMFCUS25_BRANCH', 'CMFCUS25_FORE_CASH_FLAG']:\n",
    "    OBU[i] =np.where(OBU[i].isna(), 'NA_TYPE', OBU[i])\n",
    "\n",
    "# 4. CORP_TYPE\n",
    "# 根據IssueLog Mega回覆: 共12類\n",
    "# 1~9, A, Z, Nan: NA_TYPE\n",
    "\n",
    "OBU['CORP_TYPE'] = np.where(OBU['CORP_TYPE'].isin(['1', '2', '3', '4', '5', '6', '7', '8', '9', 'Z', 'A']), OBU['CORP_TYPE'], 'NA_TYPE')\n",
    "\n",
    "\n",
    "# 全部Dummy變數\n",
    "X_Dummy = ['CMFCUS1_BIRTH_DATE', 'CMFCUS1_JOB_TITLE', 'CMFCUS1_Q_ID','CMFCUS1_PURPOSE', 'CMFCUS1_DERIVATIVE', 'CMFCUS1_TITLE_CODE', \n",
    "           'CUST_TYP', 'LN_TYP', 'CONFIRM_TYPE', 'CORP_TYPE', 'CMFCUS25_BRANCH', 'CMFCUS25_FORE_CASH_FLAG', 'CHANNEL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBU['SAR'] = LabelEncoder().fit_transform(OBU['SAR'])\n",
    "X_col = X_Categorical + X_Numerical + X_Ordinal + X_Dummy\n",
    "\n",
    "X = OBU[X_col] \n",
    "\n",
    "# 轉換dummy變數\n",
    "for each in X_Dummy:\n",
    "    dummies = pd.get_dummies(X.loc[:, each], prefix=each)\n",
    "    X = pd.concat([X, dummies], axis=1)\n",
    "    X = X.drop(each, axis=1) # delete original Dummy column\n",
    "\n",
    "y = OBU['SAR'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) #stratify確保訓練資料SAR比例一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Parameter Tuning\n",
    "Input the parameters we would like to use grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EY補充\n",
    "# 下面程式碼為選擇需要進行Tuning的參數\n",
    "# 詳細模型可使用參數請參考官方網頁: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n",
    "\n",
    "# 參數調整\n",
    "eval_metric = ['error', 'logloss', 'aucpr']\n",
    "\n",
    "space={# parameters we would like to tune\n",
    "       'eta': hp.uniform('eta', 0.2, 0.6),\n",
    "       'max_depth': hp.quniform('max_depth', 2, 6, 1), #調整樹的深度\n",
    "       'min_child_weight': hp.quniform('min_child_weight', 0, 15, 1), #調整子集合權重\n",
    "       'max_delta_step': hp.quniform('max_delta_step', 0, 10, 1),\n",
    "       'subsample': hp.uniform('subsample', 0.5, 1), #調整重覆抽樣比例\n",
    "       'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "       'gamma': hp.quniform('gamma', 0, 20, 1),\n",
    "       'reg_alpha': hp.quniform('reg_alpha', 20, 180, 1), #alpha值設定\n",
    "       'reg_lambda': hp.uniform('reg_lambda', 0, 1), #beta值設定\n",
    "       'scale_pos_weight': hp.quniform('scale_pos_weight', 50, 120, 1), # 應變數權重調整NO_SAR : SAR = 1:75\n",
    "       'eval_metric': hp.choice('eval_metric', eval_metric), \n",
    "       'n_estimators': hp.quniform('n_estimators', 100, 200, 10) \n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EY補充\n",
    "# 下面程式碼為定義參數Tuning之函式\n",
    "\n",
    "def objective(space):\n",
    "    model = xgb.XGBClassifier(eta=space['eta'],\n",
    "                              max_depth=int(space['max_depth']), \n",
    "                              min_child_weight=int(space['min_child_weight']), \n",
    "                              max_delta_step=int(space['max_delta_step']),\n",
    "                              subsample=space['subsample'], \n",
    "                              colsample_bytree=space['colsample_bytree'],\n",
    "                              gamma=space['gamma'],\n",
    "                              reg_alpha=int(space['reg_alpha']),\n",
    "                              reg_lambda=space['reg_lambda'],\n",
    "                              scale_pos_weight=int(space['scale_pos_weight']),\n",
    "                              eval_metric=space['eval_metric'], # aucpr, logloss, error\n",
    "                              n_estimators=int(space['n_estimators']),\n",
    "                              \n",
    "                              # static parameters\n",
    "                              objective='binary:logistic',\n",
    "                              random_state=1) \n",
    "    \n",
    "    evaluation = [(X_train, y_train), (X_test, y_test)]\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=evaluation, early_stopping_rounds=30, verbose=False) # verbose=False would prevent printing each iteration outcome\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # EY補充\n",
    "    # 下面recall score, roc, accuracy & f1 score 為定義模型好壞之指標(loss function)\n",
    "    # 此處詳列多個不同指標，實際進行參數Tuning時僅需選擇其一\n",
    "    recall_score = sklearn.metrics.recall_score(y_test, y_pred)\n",
    "    roc_auc_score = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test, y_pred > 0.5)\n",
    "    f1_score = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    \n",
    "    # print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "    # print(\"SCORE: \", recall_score)\n",
    "    # for the loss function: \n",
    "    # accuracy: -accuracy\n",
    "    # recall: 1-recall\n",
    "    # precision: 1-precision\n",
    "    # roc_auc_score: 1-roc-auc_score\n",
    "    \n",
    "    # EY補充\n",
    "    # 下面程式碼之目的為選擇定義模型好壞之指標(loss function)\n",
    "    # 使用者也可以透過相乘的方式使用複合性指標\n",
    "    return {'loss': (1-recall_score) * (1-accuracy),  # (1-recall_score) * (1-accuracy); 1-recall_score, -accuracy, -f1_score\n",
    "            'status': hyperopt.STATUS_OK} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調參：計算最適合的參數\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "# EY補充\n",
    "# 下面程式碼為實際執行參數Tuning\n",
    "# max_evals為設定迭代之次數，須注意迭代次數愈大，程式碼執行時間愈久\n",
    "best_hyperparams = hyperopt.fmin(fn = objective, \n",
    "                                 space = space, \n",
    "                                 algo = hyperopt.tpe.suggest, \n",
    "                                 max_evals=100, # iteration number\n",
    "                                 trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看最佳解之參數結果\n",
    "print(\"The best hyperparameters are : \", '\\n')\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行模型進行交叉驗證\n",
    "model = xgb.XGBClassifier(booster='gbtree', \n",
    "                          \n",
    "                          # Model Parameters\n",
    "                          n_estimators=int(best_hyperparams['n_estimators']), # The number of rounds for boosting\n",
    "                          eta=best_hyperparams['eta'],\n",
    "                          max_depth=int(best_hyperparams['max_depth']), # default=6\n",
    "                          min_child_weight=int(best_hyperparams['min_child_weight']), # default=1; The larger the min_child_weight is, the more conservative the algorithm will be\n",
    "                          max_delta_step=int(best_hyperparams['max_delta_step']),\n",
    "                          subsample=best_hyperparams['subsample'], # default=1; subsample ratio of the training instances.\n",
    "                          colsample_bytree=best_hyperparams['colsample_bytree'], # default=1, how many columns used in each tree training\n",
    "                          scale_pos_weight=int(best_hyperparams['scale_pos_weight']), # adjust y value weights\n",
    "                          \n",
    "                          # Learning Parameters\n",
    "                          objective='binary:logistic',\n",
    "                          gamma=best_hyperparams['gamma'], # default=0; the larger gamma is, the more conservative the algorithm will be\n",
    "                          reg_lambda=best_hyperparams['reg_lambda'], # default=1; L2 regularization term on weights. Increasing this value will make model more conservative\n",
    "                          reg_alpha=int(best_hyperparams['reg_alpha']), # default=1; L1 regularization\n",
    "                          random_state=1, \n",
    "                          eval_metric=eval_metric[best_hyperparams['eval_metric']])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "print('Recall: ', cross_val_score(model, X, y, cv=skf, scoring='recall')) # default scoring='accuracy'\n",
    "print('Accuracy: ', cross_val_score(model, X, y, cv=skf, scoring='accuracy')) # default scoring='accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 模型指標確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行模型並進行交叉驗證並檢驗AUC之穩定度\n",
    "model = xgb.XGBClassifier(booster='gbtree', \n",
    "                          \n",
    "                          # Model Parameters\n",
    "                          n_estimators=int(best_hyperparams['n_estimators']), # The number of rounds for boosting\n",
    "                          eta=best_hyperparams['eta'],\n",
    "                          max_depth=int(best_hyperparams['max_depth']), # default=6\n",
    "                          min_child_weight=int(best_hyperparams['min_child_weight']), # default=1; The larger the min_child_weight is, the more conservative the algorithm will be\n",
    "                          max_delta_step=int(best_hyperparams['max_delta_step']),\n",
    "                          subsample=best_hyperparams['subsample'], # default=1; subsample ratio of the training instances.\n",
    "                          colsample_bytree=best_hyperparams['colsample_bytree'], # default=1, how many columns used in each tree training\n",
    "                          scale_pos_weight=int(best_hyperparams['scale_pos_weight']), # adjust y value weights\n",
    "                          \n",
    "                          # Learning Parameters\n",
    "                          objective='binary:logistic',\n",
    "                          gamma=best_hyperparams['gamma'], # default=0; the larger gamma is, the more conservative the algorithm will be\n",
    "                          reg_lambda=best_hyperparams['reg_lambda'], # default=1; L2 regularization term on weights. Increasing this value will make model more conservative\n",
    "                          reg_alpha=int(best_hyperparams['reg_alpha']), # default=1; L1 regularization\n",
    "                          random_state=1, \n",
    "                          eval_metric=eval_metric[best_hyperparams['eval_metric']])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "print('Recall: ', cross_val_score(model, X, y, cv=skf, scoring='recall')) # default scoring='accuracy'\n",
    "print('Accuracy: ', cross_val_score(model, X, y, cv=skf, scoring='accuracy')) # default scoring='accuracy'\n",
    "print('------------------------------------------------------------')\n",
    "# 檢查Confidence Interval on AUC\n",
    "auc_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')\n",
    "print(auc_scores)\n",
    "print(\"AUC Confidence Interval: %0.2f (+/- %0.2f)\" % (auc_scores.mean(), auc_scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型結果檢視(Presentation)\n",
    "OBU['FINAL_CRR'] = pd.Categorical(OBU['FINAL_CRR'], ['H', 'M', 'L'])\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Record Predict Outcome\n",
    "    OBU.loc[test_index, 'pred'] = model.predict(X_test)\n",
    "    # Record Probability\n",
    "    OBU.loc[test_index, 'pred_proba'] = model.predict_proba(X_test)[:, 1].astype(float)\n",
    "    print(OBU.iloc[test_index].groupby(['FINAL_CRR', 'SAR'])['pred_proba'].mean().reset_index().sort_values(by=['FINAL_CRR', 'SAR'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型結果檢視(Presentation)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Record Predict Outcome\n",
    "    OBU.loc[test_index, 'pred'] =model.predict(X_test)\n",
    "    # Record Probability\n",
    "    OBU.loc[test_index, 'pred_proba'] = model.predict_proba(X_test)[:, 1].astype(float)\n",
    "    print(OBU.iloc[test_index].groupby(['FINAL_CRR'])['pred_proba'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型指標檢視：Accuracy, Recall, AUC\n",
    "confusion_matrix = pd.crosstab(OBU['SAR'], OBU['pred'], rownames=['Actual'], \n",
    "                              colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "print('Accuracy: ', metrics.accuracy_score(OBU['SAR'], OBU['pred']))\n",
    "plt.show()\n",
    "\n",
    "recall_score = metrics.recall_score(OBU['SAR'], OBU['pred'], pos_label=1)\n",
    "precision_score = metrics.precision_score(OBU['SAR'], OBU['pred'], pos_label=1)\n",
    "print('Recall Score: ', recall_score)\n",
    "print('Precision Score: ', precision_score)\n",
    "\n",
    "\n",
    "print('Area Under Curve: ', roc_auc_score(OBU['SAR'], OBU['pred_proba']))\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(OBU['SAR'], OBU['pred_proba'], pos_label=1)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='ROC Curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型指標檢視(Presentation)\n",
    "print(sklearn.metrics.classification_report(OBU['SAR'], OBU['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 XGBoost Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 重要欄位檢視；僅代表欄位被選擇之次數，不代表指向性(正負)\n",
    "'''\n",
    "\"weight\" is the number of times a features appears in a tree\n",
    "\"gain\" is the average gain of splits which use the feature\n",
    "\"cover\" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n",
    "'''\n",
    "\n",
    "\n",
    "xgb.plot_importance(model, max_num_features=15, importance_type='gain') # default: importance_type='weight'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 重要欄位檢視(前15名)；僅代表欄位被選擇之次數，不代表指向性(正負)\n",
    "im = pd.DataFrame({'Column': X.columns, 'Importance': model.feature_importances_})\n",
    "im = im.sort_values(by='Importance', ascending=False)\n",
    "im.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看SAR分佈\n",
    "OBU['SAR'].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL_CRR & SAR的數量分佈\n",
    "OBU.groupby(['FINAL_CRR', 'SAR'])['CUST_ID'].count().reset_index().sort_values(by=['FINAL_CRR', 'SAR'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看FINAL_CRR分佈\n",
    "OBU.groupby(['FINAL_CRR'])['CUST_ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix 2\n",
    "輸出機率數值供切分點分析  \n",
    "計算當機率切分為40%~60%時，F1-Score or Recall Score 之最佳化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = OBU.loc[:, ['pred_proba', 'FINAL_CRR', 'SAR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40, 66, 5):\n",
    "    test['pred'] = np.where(test['pred_proba'] >= i/100, 1, 0)\n",
    "    print('機率切分為', i, '%:')\n",
    "    print('Recall Score: ', metrics.recall_score(test['SAR'], test['pred']))\n",
    "    print('Accuracy: ', metrics.accuracy_score(test['SAR'], test['pred']))\n",
    "    print('F1 Score: ', metrics.f1_score(test['SAR'], test['pred']))\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
